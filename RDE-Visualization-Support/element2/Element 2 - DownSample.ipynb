{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample the data\n",
    "Need to downsample the data because it is too large to work with.  First we will look at all the data points that are within 500m of RSE #153 (which we determined was an RSE that has activity around it and is still far enough away from other RSEs to make the assumption of message uniqueness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "#function to get the distance between two lat/long points\n",
    "#return kilometers\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "#function to round a number to the nearest base\n",
    "#Example: numRound(12, 10) = 10, numRound(18, 10) = 20\n",
    "def numRound(x, base=5):\n",
    "    return int(base * round(float(x)/base))\n",
    "\n",
    "#Functions to support multiprocessing\n",
    "#Source: https://gist.github.com/yong27/7869662\n",
    "def _apply_df_rse(args):\n",
    "    df = args\n",
    "    return df.apply(lambda row: haversine(point153_lon, point153_lat, row[7]*10**-7, row[6]*10**-7), axis=1)\n",
    "\n",
    "def apply_by_multiprocessing_rse(df):\n",
    "    pool = multiprocessing.Pool(processes=8)\n",
    "    result = pool.map(_apply_df_rse, [(d)\n",
    "            for d in np.array_split(df, 8)])\n",
    "    pool.close()\n",
    "    return pd.concat(list(result),axis=0)\n",
    "\n",
    "def _apply_df_p1(args):\n",
    "    df = args\n",
    "    return df.apply(lambda row: haversine(point153_lon, point153_lat, row[8], row[7]), axis=1)\n",
    "\n",
    "def apply_by_multiprocessing_p1(df):\n",
    "    pool = multiprocessing.Pool(processes=8)\n",
    "    result = pool.map(_apply_df_p1, [(d)\n",
    "            for d in np.array_split(df, 8)])\n",
    "    pool.close()\n",
    "    return pd.concat(list(result),axis=0)\n",
    "\n",
    "\n",
    "point153_lon = -83.747333\n",
    "point153_lat = 42.289141\n",
    "\n",
    "path_to_rse_bsm = 'RSE_BSM.csv/RSE BSM.csv'\n",
    "path_to_bsmp1 = 'mnt/win/RDE Development/Release 3/data environments/2 months safety pilot data/sent August 2015/April 2013/DAS_1_BSM_Data/april_BsmP1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#ESTIMATED TIME 11s per chunk * 125 chunks = ~23mins\n",
    "#ACTUAL TIME 23min 40s\n",
    "#Chunk the csv\n",
    "rse_bsm_chunks = pd.read_csv(path_to_rse_bsm, header=None, chunksize=10**6)\n",
    "for chunk in rse_bsm_chunks:\n",
    "    #Get dataframe from chunk and add distance column\n",
    "        #Note have to scale lon/lat since they are 1/10th microdegree\n",
    "    chunk['distance from 153'] = apply_by_multiprocessing_rse(chunk)\n",
    "    close_points = chunk.loc[chunk['distance from 153'] <= 0.5]\n",
    "\n",
    "    #If any rows exist within 1km of 153 then write to our file\n",
    "    if(len(close_points) > 0):\n",
    "        close_points.to_csv('data/rse_bsm_min.csv',index=False, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#ESTIMATED TIME 5s per chunk * 1565 chunks = ~290mins\n",
    "#ACTUAL TIME 4h 58min 47s\n",
    "#Chunk the csv\n",
    "bsmp1_chunks = pd.read_csv(path_to_bsmp1, header=None, chunksize=10**6)\n",
    "for chunk in bsmp1_chunks:\n",
    "    #Get dataframe from chunk and add distance column\n",
    "    chunk['distance from 153'] = apply_by_multiprocessing_p1(chunk)\n",
    "    close_points = chunk.loc[chunk['distance from 153'] <= 0.5]\n",
    "\n",
    "    #If any rows exist within 1km of 153 then write to our file\n",
    "    if(len(close_points) > 0):\n",
    "        close_points.to_csv('data/bsmp1_min.csv',index=False, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hexbin Downsample\n",
    "Now that we have the data downsampled and were able to explore it, we want to create a hexbin map.  In order to do this we only need the lat/lon of each message that was sent and the lat/lon of each message received.  Lets downsample the previous file to get that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Downsample the data to just lat/long so that we can try plotting it on a map\n",
    "\n",
    "#Chunk the csv\n",
    "rse_chunks = pd.read_csv('data/rse_bsm_min.csv', header=None, chunksize=10**6)\n",
    "for chunk in rse_chunks:\n",
    "    small_chunk = chunk.ix[:,6:7]\n",
    "    \n",
    "    small_chunk.to_csv('data/rse_latlong.csv', index=False, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Chunk the csv\n",
    "p1_chunks = pd.read_csv('data/bsmp1_min.csv', header=None, chunksize=10**6)\n",
    "for chunk in p1_chunks:\n",
    "    small_chunk = chunk.ix[:,7:8]\n",
    "    \n",
    "    small_chunk.to_csv('data/p1_latlong.csv', index=False, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets down sample to 5 sigfigs for both p1 and rse\n",
    "df = pd.read_csv('data/rse_latlong.csv', header=None)\n",
    "df.columns = ['lat', 'lon']\n",
    "df2 = pd.read_csv('data/p1_latlong.csv', header=None)\n",
    "df2.columns = ['lat', 'lon']\n",
    "def lower_precision_rse(x):\n",
    "    return round(x*10**-7, 5)\n",
    "def lower_precision_p1(x):\n",
    "    return round(x, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp1 = df.applymap(lower_precision_rse).groupby(['lat','lon']).size().reset_index().rename(columns={0:'count'})\n",
    "temp2 = df2.applymap(lower_precision_p1).groupby(['lat','lon']).size().reset_index().rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp1.to_csv('data/rse_latlon_min.csv', index=False)\n",
    "temp2.to_csv('data/p1_latlon_min.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Heatmap DownSample\n",
    "Next need to downsample to get the data for the heatmap.  For this we need count of messages received and sent in each distance/speed bin.  Distnace binned to 100m, speed binned to 10mph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read the previously downsampled files into pandas\n",
    "p1_df = pd.read_csv('data/bsmp1_min.csv', header=None)\n",
    "rse_df = pd.read_csv('data/rse_bsm_min.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create speed column that is in mph binned to 10mph\n",
    "p1_df['speed'] = p1_df[10].apply(lambda x: numRound(x*2.23694, 10))\n",
    "rse_df['speed'] = rse_df[11].apply(lambda x: numRound(x*.02*2.23694, 10))\n",
    "\n",
    "#Create distance column that is in meters binned to 50meters\n",
    "p1_df['distance'] = p1_df[19].apply(lambda x: numRound(x*1000, 50))\n",
    "rse_df['distance'] = rse_df[25].apply(lambda x: numRound(x*1000, 50))\n",
    "\n",
    "#Find the count in each speed/distance bin and then create a column with that count\n",
    "p1_temp = p1_df.groupby(['speed', 'distance']).size().reset_index(name = 'count')\n",
    "rse_temp = rse_df.groupby(['speed', 'distance']).size().reset_index(name = 'count')\n",
    "\n",
    "#Add an indicator of which file the count came from\n",
    "p1_temp['file'] = 'p1'\n",
    "rse_temp['file'] = 'rse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write the two dataframes to a single csv file to be used in the viz\n",
    "p1_temp.to_csv('data/heat.csv', index=False)\n",
    "rse_temp.to_csv('data/heat.csv', index=False, header=False, mode='a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
